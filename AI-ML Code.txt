from urllib import request
from bs4 import  BeautifulSoup as bs

url="https://www.investopedia.com/terms/d/deep-learning.asp"
htmldoc=request.urlopen(url)
print(htmldoc)
object=bs(htmldoc,'html.parser')
print(object)

paragraph=object.find_all('p')
print(paragraph)

allparagraph=""
for paragraphs in paragraph:
    allparagraph += paragraphs.text 	
print(allparagraph)

import re
allparagraph_cleandata=re.sub(r'\[[0-9]*\]',' ', allparagraph)
allparagraph_cleandata=re.sub(r'\s+',' ', allparagraph_cleandata)
allparagraph_cleandata=re.sub(r'[^a-zA-Z]', ' ', allparagraph_cleandata)
allparagraph_cleandata=re.sub(r'\s+', ' ', allparagraph_cleandata)
print(allparagraph_cleandata)

import nltk
token=nltk.sent_tokenize(allparagraph_cleandata)
nltk.download('stopwords')
word_token=nltk.word_tokenize(allparagraph_cleandata)
print(word_token)

word_frequencies = {}
for word in word_token:
    if word not in stopwords:
        if word not in word_frequencies.keys():
            word_frequencies[word] = 1
        else:
            word_frequencies[word] += 1
print(word_frequencies ) 
maximum_frequency_word =max(word_frequencies.values())
for word in word_frequencies.keys():
    word_frequencies[word]=(word_frequencies[word]/maximum_frequency_word)
print(word_frequencies)

sentence_scores={}
for sentence in token:
    for word in nltk.word_tokenize(sentence.lower()):
        if word in word_frequencies.keys():
            if (len(sentence.split(' '))) < 30:
                if sentence not in sentence_scores.keys():
                    sentence_scores[sentence] = word_frequencies[word]
                else:
                     sentence_scores[sentence] += word_frequencies[word]
print(sentence_scores)

import heapq
summary=heapq.nlargest(20,sentence_scores,key=sentence_scores.get)
print(summary)

import heapq
summary=heapq.nlargest(40,sentence_scores,key=sentence_scores.get)
print(summary)

